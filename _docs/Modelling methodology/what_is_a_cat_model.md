---
title: What is a catastrophe model?
category: Modelling methodology
order: 1
---

Catastrophe models are used extensively in the industry today to estimate expected losses from natural disasters. Catastrophe models output “Exceedance Probability” curves, i.e. a probability distribution of losses that will be sustained by an insurance company in any given year, together with an “annual average loss” and standard deviation.  Given the paucity in historical losses for extreme events from which to build actuarial based models, catastrophe models take a bottom-up approach from scientific first principles to estimate the risk. The anatomy of a typical catastrophe model is shown below:

![Anatomy of a catastrophe model](/images/anatomy_of_a_cat_model.png)

The first task is to generate an “event set” of all possible events that may occur, along with their intensity and probability across a long-enough time period to encapsulate a comprehensive distribution of even the most extreme events. A 10,000 year simulation is often used. The goal is not to recreate the last 10,000 years of history, but to simulate 10,000 years of activity equivalent to “current” conditions. Each event has a probability of occurrence within the simulated time period. Models often employ a “boiling down” process to optimise the run-times of their models by combining very similar events together, including their probability of occurrence.  This maintains the representativeness of the ultimate event set to be consistent with the original event set in terms of losses and the geographical distribution of loss, but is faster for the user to run.

For each event a hazard footprint will be generatehttps://www.google.co.uk/d, which calculates an appropriate hazard metric which correlates to damage at each point in a grid across the entire area effected by an event. For example, this is may be the maximum 3-second peak gust experienced at every location by a windstorm during the course of that windstorm. Time-stepping models are used which simulate the storm and its windfield say every 15 minutes throughout the entire lifecycle of the storm, which may be many hours in duration. Topography, surface roughness, soil and geological information must all be taken into account, as the model is representing the hazard at the surface of the ground. The maximum peak-gust windspeed experienced is stored as the “hazard footprint” provided by a catastrophe model. 

Vulnerability curves link the hazard metric  (e.g. 3-second peak gust or flood depth) to a Mean Damage Ratio (MDR), ie the proportion of the total value (e.g. in terms of replacement cost) that would be a loss for the asset being analysed.  In reality, properties exhibit a high amount of variability in their damage to the same hazard e.g. windspeed due to many unknown and un-modellable factors, even when located very close to each other. This is accounted for in an uncertainty distribution around the mean damage ratio at each hazard point, also known as “secondary uncertainty”. Models will often define different “vulnerability zones” across a region to account for different building practices or building codes. 

A “financial module” calculates losses after taking into account the impact of insurance company policy terms & conditions, e.g. deductibles, limits, reinsurance etc, to provide the net loss that the re/insurance entity will ultimately be responsible for. 
The re/insurance company enters a list of all the policies it has underwritten with information about the location and characteristics, e.g. residential or commercial usage, age, construction material, height, and value (replacement cost) of the building, contents and policy terms & conditions. The catastrophe model will then run the entire event set across the portfolio, and calculate a loss from every event in the model to every policy. This produces an event loss table. These event losses are then ordered in terms of magnitude from largest to smallest to generate the “Exceedance Probability” curve for the number of years the model simulates, e.g. 10,000 years. This describes the probability that various levels of loss will be exceeded, in terms of return periods or annual probabilities of a loss being exceeded. For example, at a 1 in 250 year return period, the probability of losses exceeding $XXXbn in any given year is 0.4%. The average annual loss is the mean value of the losses from all events, calculated by summing up all the event losses and averaging over the number of years simulated by the model. The below chart shows a comparison of Europe Windstorm EP Curves produced by various catastrophe models (IUA 2015 – Europe Windstorm Vendor Model Comparison).

![Catastrophe model exceedence curves](/images/ep-curve-example.png)


Catastrophe models typically cover single peril-region combinations, e.g. Europe windstorm, Japanese earthquakes. Whilst AALs from each peril-region combination analysis can be added together, EP curves cannot, and must be recalculated after different peril-region analyses have been “grouped” together. This is because of the diversifying nature of writing risk in different, uncorrelated regions, or conversely because two portfolios have a very similar risk profile and are correlated, and therefore the combined return-period risk may be more or less than the sum of the two. 

Catastrophe model loss results output varies considerably between different developers, due to differences in data, assumptions, modelling techniques etc. Users can validate models against observational data, losses, and claims data if they have them at a detailed level, e.g. an insurance companies own data. This takes significant expertise, for example if comparing model windspeeds against observed windspeeds, care must be taken to account for the fact that windspeed observations are usually recorded at a standard height above ground level, e.g. 10m above the ground level in open-terrain conditions. However, the catastrophe model is simulating surface windspeeds, and incorporating the effect of surface roughness and upwind obstacles such as trees and buildings in this calculation. Also the catastrophe model will probably be calculating each event’s peak winds for every cell of e.g. 1 km² across a grid. Data only exists at lower return periods of course, therefore qualitative expert judgement will be used to evaluate a model appropriateness and fit-for-purpose.